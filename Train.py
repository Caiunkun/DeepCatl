import os
import xlwt
os.environ["CUDA_VISIBLE_DEVICES"] = '0'

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as loader
import math
import numpy as np

from tqdm import tqdm
from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_curve, auc
from torch.utils.data import random_split
from Dataset.script import SSDataset_690
from fightingcv_attention.attention.ECAAttention import ECAAttention

class Constructor:

    def __init__(self, model, model_name='d_ssca'):

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = model.to(device=self.device)
        self.model_name = model_name
        self.optimizer = optim.Adam(self.model.parameters())
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=self.optimizer, patience=5, verbose=1)
        self.loss_function = nn.BCELoss()

        self.batch_size = 64
        self.epochs = 14

    def learn(self, TrainLoader, ValidateLoader):

        path = os.path.abspath(os.curdir)
        for epoch in range(self.epochs):
            self.model.train()
            ProgressBar = tqdm(TrainLoader)
            for data in ProgressBar:
                self.optimizer.zero_grad()
                ProgressBar.set_description("Epoch %d" % epoch)
                seq, shape, label = data
                output = self.model(seq.unsqueeze(1).to(self.device), shape.unsqueeze(1).to(self.device))   #è®­ç»ƒ
                loss = self.loss_function(output, label.float().to(self.device))
                ProgressBar.set_postfix(loss=loss.item())
                loss.backward()
                self.optimizer.step()
            valid_loss = []
            self.model.eval()
            with torch.no_grad():
                for valid_seq, valid_shape, valid_labels in ValidateLoader:
                    valid_output = self.model(valid_seq.unsqueeze(1).to(self.device), valid_shape.unsqueeze(1).to(self.device))
                    valid_labels = valid_labels.float().to(self.device)
                    valid_loss.append(self.loss_function(valid_output, valid_labels).item())
                valid_loss_avg = torch.mean(torch.Tensor(valid_loss))
                self.scheduler.step(valid_loss_avg)
        torch.save(self.model.state_dict(), path + '\\' + self.model_name + '.pth')
        print('\n---Finish Learn---\n')

    def inference(self, TestLoader):

        path = os.path.abspath(os.curdir)
        self.model.load_state_dict(torch.load(path + '\\' + self.model_name + '.pth', map_location='cpu'))

        predicted_value = []
        ground_label = []


        # model.eval()çš„ä½œç”¨æ˜¯ä¸å¯ç”¨ Batch Normalization å’Œ Dropoutã€‚
        self.model.eval()
        # éå†æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼Œæ¯æ¬¡è¿­ä»£è·å–ä¸€ä¸ªæµ‹è¯•æ ·æœ¬çš„åºåˆ—æ•°æ®ã€å½¢çŠ¶ä¿¡æ¯å’Œæ ‡ç­¾ã€‚
        for seq, shape, label in TestLoader:
            output = self.model( seq.unsqueeze(1).to(self.device), shape.unsqueeze(1).to(self.device) )
            # """ To scalar"""
            # çœ‹ä¸æ‡‚
            predicted_value.append(output.squeeze(dim=0).squeeze(dim=0).detach().cpu().numpy())
            ground_label.append(label.squeeze(dim=0).squeeze(dim=0).detach().numpy())

        print('\n---Finish Inference---\n')
        return predicted_value, ground_label

    def measure(self, predicted_value, ground_label):
        accuracy = accuracy_score(y_pred=np.array(predicted_value).round(), y_true=ground_label)
        roc_auc = roc_auc_score(y_score=predicted_value, y_true=ground_label)

        precision, recall, _ = precision_recall_curve(probas_pred=predicted_value, y_true=ground_label)
        pr_auc = auc(recall, precision)

        print('\n---Finish Measure---\n')

        return accuracy, roc_auc, pr_auc

    def run(self, samples_file_name, ratio=0.8):

        Train_Validate_Set = SSDataset_690(samples_file_name, False)          #åŠ è½½æ•°æ®  è¿”å›æ•°æ®é›†å¯¹è±¡ç”¨äºè®­ç»ƒå’ŒéªŒè¯

        """divide Train samples and Validate samples"""                       #åˆ’åˆ†æ•°æ®é›†
        Train_Set, Validate_Set = random_split(dataset=Train_Validate_Set,
                                               lengths=[math.ceil(len(Train_Validate_Set) * ratio),
                                                        len(Train_Validate_Set) -
                                                        math.ceil(len(Train_Validate_Set) * ratio)],
                                               generator=torch.Generator().manual_seed(0))
        # ä¸€ã€ğŸŒ±ğŸ“š DataLoaderçš„å‚æ•°è¯´æ˜ ğŸ“šğŸŒ±
        # dataset(å¿…éœ€): ç”¨äºåŠ è½½æ•°æ®çš„æ•°æ®é›†ï¼Œé€šå¸¸æ˜¯torch.utils.data.Datasetçš„å­ç±»å®ä¾‹ã€‚
        # batch_size(å¯é€‰): æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®æ ·æœ¬æ•°ã€‚é»˜è®¤å€¼ä¸º1ã€‚             å½“ä¸€ä¸ªEpochçš„æ ·æœ¬ï¼ˆä¹Ÿå°±æ˜¯æ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬ï¼‰æ•°é‡å¯èƒ½å¤ªè¿‡åºå¤§ï¼ˆå¯¹äºè®¡ç®—æœºè€Œè¨€ï¼‰ï¼Œå°±éœ€è¦æŠŠå®ƒåˆ†æˆå¤šä¸ªå°å—ï¼Œä¹Ÿå°±æ˜¯å°±æ˜¯åˆ†æˆå¤šä¸ªBatch æ¥è¿›è¡Œè®­ç»ƒã€‚
        # shuffle(å¯é€‰): æ˜¯å¦åœ¨æ¯ä¸ªå‘¨æœŸå¼€å§‹æ—¶æ‰“ä¹±æ•°æ®ã€‚é»˜è®¤ä¸ºFalseã€‚
        # sampler(å¯é€‰): å®šä¹‰ä»æ•°æ®é›†ä¸­æŠ½å–æ ·æœ¬çš„ç­–ç•¥ã€‚å¦‚æœæŒ‡å®šï¼Œåˆ™å¿½ç•¥shuffleå‚æ•°ã€‚
        # batch_sampler(å¯é€‰): ä¸samplerç±»ä¼¼ï¼Œä½†ä¸€æ¬¡è¿”å›ä¸€ä¸ªæ‰¹æ¬¡çš„ç´¢å¼•ã€‚ä¸èƒ½ä¸batch_sizeã€shuffleå’ŒsampleråŒæ—¶ä½¿ç”¨ã€‚
        # num_workers(å¯é€‰): ç”¨äºæ•°æ®åŠ è½½çš„å­è¿›ç¨‹æ•°é‡ã€‚é»˜è®¤ä¸º0ï¼Œæ„å‘³ç€æ•°æ®å°†åœ¨ä¸»è¿›ç¨‹ä¸­åŠ è½½ã€‚
        # collate_fn(å¯é€‰): å¦‚ä½•å°†å¤šä¸ªæ•°æ®æ ·æœ¬æ•´åˆæˆä¸€ä¸ªæ‰¹æ¬¡ã€‚é€šå¸¸ä¸éœ€è¦æŒ‡å®šã€‚
        # drop_last(å¯é€‰): å¦‚æœæ•°æ®é›†å¤§å°ä¸èƒ½è¢«æ‰¹æ¬¡å¤§å°æ•´é™¤ï¼Œæ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„æ‰¹æ¬¡ã€‚é»˜è®¤ä¸ºFalseã€‚

        #   è®­ç»ƒé›†
        TrainLoader = loader.DataLoader(dataset=Train_Set, drop_last=True,
                                        batch_size=self.batch_size, shuffle=True, num_workers=0)
        #   æµ‹è¯•é›†
        ValidateLoader = loader.DataLoader(dataset=Validate_Set, drop_last=True,
                                           batch_size=self.batch_size, shuffle=False, num_workers=0)
        #   éªŒè¯é›†
        TestLoader = loader.DataLoader(dataset=SSDataset_690(samples_file_name, True),
                                       batch_size=1, shuffle=False, num_workers=0)


        #   å¼€å§‹è®­ç»ƒ
        self.learn(TrainLoader, ValidateLoader)


        predicted_value, ground_label = self.inference(TestLoader)

        accuracy, roc_auc, pr_auc = self.measure(predicted_value, ground_label)

        print('\n---Finish Run---\n')
        print('acc,roc,prauc',accuracy,roc_auc,pr_auc)

        return accuracy, roc_auc, pr_auc


from models.DanQ import DanQ
from models.D_SSCA import d_ssca
from models.CRPTS import crpts  
from models.DeepCatl import deepcatl


Train = Constructor(model=deepcatl())
model=deepcatl()
strings = ["wgEncodeAwgTfbsBroadDnd41CtcfUniPk"]





listacc = []
listauc = []
listprauc = []


for i in range(len(strings)):
    print("'"+strings[i]+"'")
    lista,listb,listc = Train.run(samples_file_name=strings[i])

    listacc.append(lista)
    listauc.append(listb)
    listprauc.append(listc)


f = xlwt.Workbook('encoding = utf-8')  # è®¾ç½®å·¥ä½œç°¿ç¼–ç 
sheet1 = f.add_sheet('sheet1', cell_overwrite_ok=True)  # åˆ›å»ºsheetå·¥ä½œè¡¨

for i in range(len(listacc)):
    sheet1.write(i, 0, listacc[i])  # å†™å…¥æ•°æ®å‚æ•°å¯¹åº” è¡Œ, åˆ—, å€¼

for i in range(len(listauc)):
    sheet1.write(i, 1, listauc[i])  # å†™å…¥æ•°æ®å‚æ•°å¯¹åº” è¡Œ, åˆ—, å€¼

for i in range(len(listprauc)):
    sheet1.write(i, 2, listprauc[i])  # å†™å…¥æ•°æ®å‚æ•°å¯¹åº” è¡Œ, åˆ—, å€¼
f.save('text.xls')  # ä¿å­˜.xlsåˆ°å½“å‰å·¥ä½œç›®å½•



